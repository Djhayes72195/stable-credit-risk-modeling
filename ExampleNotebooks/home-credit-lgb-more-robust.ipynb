{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:14:56.335078Z","iopub.status.busy":"2024-03-30T16:14:56.334701Z","iopub.status.idle":"2024-03-30T16:14:56.370288Z","shell.execute_reply":"2024-03-30T16:14:56.368992Z","shell.execute_reply.started":"2024-03-30T16:14:56.335043Z"},"trusted":true},"outputs":[],"source":["# https://blog.csdn.net/s09094031/article/details/92428209?app_version=6.3.1&csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%2292428209%22%2C%22source%22%3A%22unlogin%22%7D&utm_source=app"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-30T16:14:56.372402Z","iopub.status.busy":"2024-03-30T16:14:56.372086Z","iopub.status.idle":"2024-03-30T16:14:59.704965Z","shell.execute_reply":"2024-03-30T16:14:59.703713Z","shell.execute_reply.started":"2024-03-30T16:14:56.372375Z"},"trusted":true},"outputs":[],"source":["import sys\n","from pathlib import Path\n","import subprocess\n","import os\n","import gc\n","from glob import glob\n","\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","from datetime import datetime\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","ROOT = Path('/Users/dustinhayes/Desktop/GitHub/stable-credit-risk-modeling/Data')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:14:59.712806Z","iopub.status.busy":"2024-03-30T16:14:59.71117Z","iopub.status.idle":"2024-03-30T16:15:01.665777Z","shell.execute_reply":"2024-03-30T16:15:01.664421Z","shell.execute_reply.started":"2024-03-30T16:14:59.712765Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n","from sklearn.base import BaseEstimator, RegressorMixin\n","from sklearn.metrics import roc_auc_score\n","import lightgbm as lgb\n","\n","from imblearn.over_sampling import SMOTE\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.impute import KNNImputer"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:15:01.669067Z","iopub.status.busy":"2024-03-30T16:15:01.668487Z","iopub.status.idle":"2024-03-30T16:15:01.682365Z","shell.execute_reply":"2024-03-30T16:15:01.681099Z","shell.execute_reply.started":"2024-03-30T16:15:01.669031Z"},"trusted":true},"outputs":[],"source":["class Pipeline:\n","\n","    def set_table_dtypes(df):\n","        \"\"\"\n","        Set data types.\n","\n","        Used in the data loading pipeline to set data types\n","        for efficiency.\n","\n","        \"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\" are special\n","        columns. case_id is the identifier for each case, WEEK_NUM indicates\n","        the week that an observation was taken, and num_group1 and num_group2\n","        are indexes used in depth=1 and depth=2 tables. Int is suitable for each.\n","\n","        date_decision is likewise special: it denotes the date that the choice\n","        to either deny or issue the loan was made.\n","\n","        Datatypes for remaining columns are selected based on the last character\n","        in the column name:\n","\n","        P - Transform DPD (Days past due) - Float\n","        M - Masking categories - String\n","        A - Transform amount - Float\n","        D - Transform date - Date\n","        T - Unspecified Transform - Not handled\n","        L - Unspecified Transform - Not handled\n","        \"\"\"\n","        for col in df.columns:\n","            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Int64))\n","            elif col in [\"date_decision\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","            elif col[-1] in (\"P\", \"A\"):\n","                df = df.with_columns(pl.col(col).cast(pl.Float64))\n","            elif col[-1] in (\"M\",):\n","                df = df.with_columns(pl.col(col).cast(pl.String))\n","            elif col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","        return df\n","\n","    def handle_dates(df):\n","        \"\"\"\n","        Convert date values to a # days difference from date decision.\n","\n","        This function locates date columns (ending in D) and converts\n","        to a # of days difference between the date decision and the date of interest.\n","        \"\"\"\n","        for col in df.columns:\n","            if col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  # Duration\n","                df = df.with_columns(pl.col(col).dt.total_days()) # Count of days\n","        # Polars handles conversion to int automatically\n","        df = df.drop(\"date_decision\", \"MONTH\")\n","        return df\n","\n","    def filter_cols(df):\n","        \"\"\"\n","        Filters columns based on # of null values and frequency of categorical values,\n","        with exceptions for specific columns.\n","\n","        Logic:\n","            - Drop if greater than 70% of the column is null and not in \"target\", \"case_id\", \"WEEK_NUM\"\n","            - If categorical and not in \"target\", \"case_id\", \"WEEK_NUM\":\n","                - Drop if column has only one unique value (not informative)\n","                - Drop if column has more than 200 unique values (high cardinality, expensive and\n","                may lead to overfitting)\n","        \"\"\"\n","        for col in df.columns:\n","            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n","                isnull = df[col].is_null().mean()\n","                if isnull > 0.7:\n","                    df = df.drop(col)\n","        \n","        for col in df.columns:\n","            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n","                freq = df[col].n_unique()\n","                if (freq == 1) | (freq > 200):\n","                    df = df.drop(col)\n","        \n","        return df"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:15:01.688896Z","iopub.status.busy":"2024-03-30T16:15:01.687932Z","iopub.status.idle":"2024-03-30T16:15:01.706025Z","shell.execute_reply":"2024-03-30T16:15:01.70438Z","shell.execute_reply.started":"2024-03-30T16:15:01.688856Z"},"trusted":true},"outputs":[],"source":["\n","class Aggregator:\n","    \"\"\"\n","    A namespace for a set of functions used to aggregate data with depth > 0.\n","\n","    Data with depth > 0 has multiple values per case_id, for instance, temporal data\n","    measured week by week related to a particular case_id. This data must be aggregated\n","    before an ML algorithm can be run on it.\n","\n","    Each of these methods returns a list of expresions which apply to different data\n","    types.\n","    \"\"\"\n","    def num_expr(df):\n","        \"\"\"\n","        Aggregate numerical data.\n","\n","        This method returns a list of expressions which calculate\n","        the max value, last (most recent) value, and the mean.\n","        \"\"\"\n","        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        \n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n","        return expr_max +expr_last+expr_mean\n","    \n","    def date_expr(df):\n","        \"\"\"\n","        Aggregate date data.\n","\n","        This method returns a list of expressions which calculate\n","        the max date, last date, and mean date. Note that dates will have\n","        been converted to an int representing # of days from decision date.\n","\n","        Question: Why are last date and max date not the same?\n","        \"\"\"\n","        cols = [col for col in df.columns if col[-1] in (\"D\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n","        return  expr_max +expr_last+expr_mean\n","    \n","    def str_expr(df):\n","        \"\"\"\n","        Aggregate categorical data.\n","\n","        This function is more confusing for me. It seems to return an expression\n","        which retrieves the \"max\" string and the \"last\" string.\n","\n","        I don't see how the \"max\" string, which I am quite sure would just\n","        be the last string when ordered alphabetically, would be useful.\n","        \"\"\"\n","        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        #expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n","        return  expr_max +expr_last#+expr_count\n","    \n","    def other_expr(df):\n","        \"\"\"\n","        Aggregate other data.\n","\n","        Returns an expression which gets the max and last values in the\n","        supplied column.\n","        \"\"\"\n","        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        return  expr_max +expr_last\n","    \n","    def count_expr(df):\n","        \"\"\"\n","        I'll have to come back to see what this does.\n","        \"\"\"\n","        cols = [col for col in df.columns if \"num_group\" in col]\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n","        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n","        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n","        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n","        return  expr_max +expr_last\n","    \n","    def get_exprs(df):\n","        exprs = Aggregator.num_expr(df) + \\\n","                Aggregator.date_expr(df) + \\\n","                Aggregator.str_expr(df) + \\\n","                Aggregator.other_expr(df) + \\\n","                Aggregator.count_expr(df)\n","\n","        return exprs"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:15:01.708353Z","iopub.status.busy":"2024-03-30T16:15:01.707355Z","iopub.status.idle":"2024-03-30T16:15:01.73173Z","shell.execute_reply":"2024-03-30T16:15:01.730856Z","shell.execute_reply.started":"2024-03-30T16:15:01.708319Z"},"trusted":true},"outputs":[],"source":["def read_file(path, depth=None):\n","    \"\"\"\n","    Reads a parquet file than performs data processing steps.\n","\n","    First, type setting is applied using Pipeline.set_table_dtypes.\n","    Then, if depth == 1 or depth == 2, indicating that multiple records may be included\n","    for each case_id, it groups the DataFrame by 'case_id' and then aggregates it using \n","    expressions generated by the get_exprs method from the Aggregator class.\n","    \"\"\"\n","    df = pl.read_parquet(path)\n","    df = df.pipe(Pipeline.set_table_dtypes)\n","    if depth in [1,2]:\n","        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n","    return df\n","\n","def read_files(regex_path, depth=None):\n","    \"\"\"\n","    Read multiple files and performs data processing steps.\n","\n","    Similar to read_file, this function read multiple files. It reads each file,\n","    performs the same aggregations, appends the result to \"chunks\", then\n","    concatenates all dfs such that a single df is returned.\n","    \"\"\"\n","    chunks = []\n","    \n","    for path in glob(str(regex_path)):\n","        df = pl.read_parquet(path)\n","        df = df.pipe(Pipeline.set_table_dtypes)\n","        if depth in [1, 2]:\n","            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","        chunks.append(df)\n","    \n","    df = pl.concat(chunks, how=\"vertical_relaxed\")\n","    df = df.unique(subset=[\"case_id\"])\n","    return df"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:15:01.734329Z","iopub.status.busy":"2024-03-30T16:15:01.733322Z","iopub.status.idle":"2024-03-30T16:15:01.751666Z","shell.execute_reply":"2024-03-30T16:15:01.75023Z","shell.execute_reply.started":"2024-03-30T16:15:01.734287Z"},"trusted":true},"outputs":[],"source":["def feature_eng(df_base, depth_0, depth_1, depth_2):\n","    \"\"\"\n","    Performs an additional feature engineering step and joins tables.\n","\n","    This functino first adds two columns to the base_df passed in: the month\n","    that the decision was made and the day that the decision was made. Both\n","    values will be recorded as an integer. \n","\n","    This function also joins the base_df with the tables passed in\n","    to depth_0, depth_1, depth_2 and returns the combination of all\n","    tables.\n","    \"\"\"\n","    df_base = (\n","        df_base\n","        .with_columns(\n","            month_decision = pl.col(\"date_decision\").dt.month(),\n","            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n","        )\n","    )\n","    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n","        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n","    df_base = df_base.pipe(Pipeline.handle_dates)\n","    return df_base"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:15:01.753619Z","iopub.status.busy":"2024-03-30T16:15:01.75327Z","iopub.status.idle":"2024-03-30T16:15:01.765547Z","shell.execute_reply":"2024-03-30T16:15:01.764126Z","shell.execute_reply.started":"2024-03-30T16:15:01.75359Z"},"trusted":true},"outputs":[],"source":["def to_pandas(df_data, cat_cols=None):\n","    \"\"\"\n","    Convert a polars df to a pandas df.\n","\n","    The main purpose of this function is to accept a polars\n","    dataframe and return a pandas dataframe. This function may\n","    be used to leverage functionality that exists in pandas but\n","    not polars.\n","    \n","    Additionally, this function accepts a parameter cat_cols,\n","    which may be used to identify which columns should be converted to the\n","    \"category\" datatype. If cat_cols is None, it is assumed that all\n","    columns with datatype \"object\" should be converted to \"category\".\n","    \"\"\"\n","    df_data = df_data.to_pandas()\n","    if cat_cols is None:\n","        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n","    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n","    return df_data, cat_cols"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:15:01.773104Z","iopub.status.busy":"2024-03-30T16:15:01.771754Z","iopub.status.idle":"2024-03-30T16:15:01.787954Z","shell.execute_reply":"2024-03-30T16:15:01.786436Z","shell.execute_reply.started":"2024-03-30T16:15:01.773057Z"},"trusted":true},"outputs":[],"source":["def reduce_mem_usage(df):\n","    \"\"\"\n","    Iterate through all the columns of a dataframe and modify the data type\n","    to reduce memory usage.\n","\n","    This function operates under the principle that a numerical column\n","    should use the more memory efficient datatype possible. For example,\n","    if the largest and smallest int in a column can be represented with int8, we should\n","    use int8 instead of a of larger type such as int16. It performs similar operations\n","    on floats such that the most memory efficient datatype is employed.\n","\n","    \"category\" and \"object\" types are skipped as they are not suitable for this\n","    sort of type casting.\n","\n","    The memory usage of the dataframe is recorded and printed before and after\n","    the operation, allowing the user to know the extent to which memory usage was\n","    optimized.\n","    \"\"\"\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","    \n","    for col in df.columns:\n","        col_type = df[col].dtype\n","        if str(col_type)==\"category\":\n","            continue\n","        \n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","        else:\n","            continue\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","    \n","    return df"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:15:01.793603Z","iopub.status.busy":"2024-03-30T16:15:01.792112Z","iopub.status.idle":"2024-03-30T16:15:01.8043Z","shell.execute_reply":"2024-03-30T16:15:01.803136Z","shell.execute_reply.started":"2024-03-30T16:15:01.793554Z"},"trusted":true},"outputs":[],"source":["TRAIN_DIR       = ROOT / \"original_parquet_files\" / \"train\"\n","TEST_DIR        = ROOT / \"original_parquet_files\" / \"test\""]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:15:01.806423Z","iopub.status.busy":"2024-03-30T16:15:01.805775Z","iopub.status.idle":"2024-03-30T16:18:03.061Z","shell.execute_reply":"2024-03-30T16:18:03.059791Z","shell.execute_reply.started":"2024-03-30T16:15:01.806384Z"},"trusted":true},"outputs":[],"source":["\n","# A dictionary object containing all training data, organized by depth.\n","data_store = {\n","    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n","    \"depth_0\": [\n","        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n","        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n","        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n","        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n","        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n","        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n","    ]\n","}"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["We now initialize the training set using the feature_eng function, which accepts the data_store unpacked into key, value pairs corresponding to the the base dataframe, depth 0 dataframes, depth 1 dataframes, and depth 2 dataframes. feature_eng will return a concatonation of all dataframes in the data_store after handling dates using Pipeline.handle_dates. We then delete the data_store and collect garbage to save memory.\n","\n","Pipeline.filter_cols is then called on the training set to remove columns with many nulls as well as categorical columns with too few or too many unique values.\n","\n","df_train is then converted to pandas. This is presumably done to take advantage of functionality that which pandas posseses but polars does not.\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:18:03.063164Z","iopub.status.busy":"2024-03-30T16:18:03.062497Z","iopub.status.idle":"2024-03-30T16:19:52.757155Z","shell.execute_reply":"2024-03-30T16:19:52.755703Z","shell.execute_reply.started":"2024-03-30T16:18:03.063129Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train data shape:\t (1526659, 861)\n","Memory usage of dataframe is 4322.75 MB\n","Memory usage after optimization is: 1528.81 MB\n","Decreased by 64.6%\n","train data shape:\t (1526659, 472)\n"]}],"source":["df_train = feature_eng(**data_store)\n","print(\"train data shape:\\t\", df_train.shape)\n","# del data_store\n","gc.collect()\n","df_train = df_train.pipe(Pipeline.filter_cols) # This removes about half of all cols\n","df_train, cat_cols = to_pandas(df_train) # Sets cat_cols to be a list of \"object\" cols in the set\n","df_train = reduce_mem_usage(df_train)\n","print(\"train data shape:\\t\", df_train.shape)\n","nums=df_train.select_dtypes(exclude='category').columns # Grab numerical cols\n","from itertools import combinations, permutations\n","#df_train=df_train[nums]\n","nans_df = df_train[nums].isna() # Boolean mask for is na\n","nans_groups={}\n","# Iterate through all numerical cols to create a dict (nans_groups)\n","# Where Keys are the indicate a number of nulls, and values are a list of\n","# columns with that many nulls.\n","for col in nums:\n","    cur_group = nans_df[col].sum() # Count number of nan\n","    try:\n","        nans_groups[cur_group].append(col) \n","    except:\n","        nans_groups[cur_group]=[col]\n","del nans_df; x=gc.collect()\n","\n","def reduce_group(grps):\n","    use = []\n","    for g in grps:\n","        mx = 0; vx = g[0]\n","        for gg in g:\n","            n = df_train[gg].nunique()\n","            if n>mx:\n","                mx = n\n","                vx = gg\n","            #print(str(gg)+'-'+str(n),', ',end='')\n","        use.append(vx)\n","        #print()\n","    print('Use these',use)\n","    return use\n","\n","def group_columns_by_correlation(matrix, threshold=0.8):\n","    # 计算列之间的相关性\n","    correlation_matrix = matrix.corr() # a col # by col # matrix, where each entry is correlation between numeric cols\n","\n","    # 分组列\n","    groups = []\n","    remaining_cols = list(matrix.columns)\n","    while remaining_cols:\n","        col = remaining_cols.pop(0)\n","        group = [col]\n","        correlated_cols = [col]\n","        for c in remaining_cols:\n","            if correlation_matrix.loc[col, c] >= threshold: # If correlation between c and col > threshold\n","                group.append(c) # Add c to group (col is already in there)\n","                correlated_cols.append(c)\n","        groups.append(group)\n","        # Remove cols that have been grouped\n","        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n","    \n","    return groups # groups will be a list where each element is the list of features that\n","                  # are have correlation greater than the threshold. We may want to explore the logic\n","                  # here a bit more on the basis of this question: If the corr between A and B and the corr\n","                  # between A and C is X, what can be said of the correlation between B and C? i.e. there\n","                  # may be a dependency on the order of the input list.\n","\n","uses=[]\n","for k,v in nans_groups.items():\n","    if len(v)>1: # If more than one feature has a particular amount of nulls\n","            Vs = nans_groups[k]\n","            #cross_features=list(combinations(Vs, 2))\n","            #make_corr(Vs)\n","            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n","            use=reduce_group(grps)\n","            uses=uses+use\n","            #make_corr(use)\n","    else:\n","        uses=uses+v\n","    print('####### NAN count =',k)\n","print(uses)\n","print(len(uses))\n","uses=uses+list(df_train.select_dtypes(include='category').columns)\n","print(len(uses))\n","df_train=df_train[uses]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:19:52.758898Z","iopub.status.busy":"2024-03-30T16:19:52.758558Z","iopub.status.idle":"2024-03-30T16:19:52.774816Z","shell.execute_reply":"2024-03-30T16:19:52.773664Z","shell.execute_reply.started":"2024-03-30T16:19:52.75887Z"},"trusted":true},"outputs":[],"source":["sample = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\")\n","device='gpu'\n","#n_samples=200000\n","DRY_RUN = True if sample.shape[0] == 10 else False   \n","if DRY_RUN:\n","    device='cpu'\n","    df_train = df_train.iloc[:50000]\n","    #n_samples=10000\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:19:52.77688Z","iopub.status.busy":"2024-03-30T16:19:52.77655Z","iopub.status.idle":"2024-03-30T16:19:53.120039Z","shell.execute_reply":"2024-03-30T16:19:53.11894Z","shell.execute_reply.started":"2024-03-30T16:19:52.776852Z"},"trusted":true},"outputs":[],"source":["data_store = {\n","    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n","    \"depth_0\": [\n","        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n","        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n","        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n","        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n","        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n","        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n","        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n","    ]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:19:53.121873Z","iopub.status.busy":"2024-03-30T16:19:53.121493Z","iopub.status.idle":"2024-03-30T16:19:53.697635Z","shell.execute_reply":"2024-03-30T16:19:53.696429Z","shell.execute_reply.started":"2024-03-30T16:19:53.121814Z"},"trusted":true},"outputs":[],"source":["df_test = feature_eng(**data_store)\n","print(\"test data shape:\\t\", df_test.shape)\n","del data_store\n","gc.collect()\n","df_test = df_test.select([col for col in df_train.columns if col != \"target\"])\n","print(\"train data shape:\\t\", df_train.shape)\n","print(\"test data shape:\\t\", df_test.shape)\n","\n","df_test, cat_cols = to_pandas(df_test, cat_cols)\n","df_test = reduce_mem_usage(df_test)\n","\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["### Feature Selection"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:19:53.699608Z","iopub.status.busy":"2024-03-30T16:19:53.699273Z","iopub.status.idle":"2024-03-30T16:19:53.705775Z","shell.execute_reply":"2024-03-30T16:19:53.704664Z","shell.execute_reply.started":"2024-03-30T16:19:53.699579Z"},"trusted":true},"outputs":[],"source":["# drop_list = ['max_empl_employedtotal_800L', 'monthsannuity_845L', 'lastactivateddate_801D', \n","#              'max_numberofoverdueinstls_725L', 'requesttype_4525192L', 'max_pmts_year_507T', \n","#              'lastrejectcommodtypec_5251769M', 'numinstpaidlate1d_3546852L', 'numinstmatpaidtearly2d_4499204L', \n","#              'max_overdueamountmaxdateyear_2T', 'max_overdueamountmaxdateyear_994T', 'twobodfilling_608L', \n","#              'maxdpdlast12m_727P', 'numinsttopaygrest_4493213L', 'currdebtcredtyperange_828A', 'maxdpdlast9m_1059P', \n","#              'numinstpaid_4499208L', 'applicationscnt_867L', 'numinstlswithoutdpd_562L', 'fourthquarter_440L', \n","#              'max_num_group1_6', 'max_safeguarantyflag_411L', 'max_dpdmaxdateyear_896T', 'numinstregularpaid_973L', \n","#              'avgdbdtollast24m_4525197P', 'numinstpaidearly5dest_4493211L', 'numinstpaidearly5dobd_4499205L', \n","#              'homephncnt_628L', 'max_role_1084L', 'max_remitter_829L', 'numrejects9m_859L', \n","#              'numinstlallpaidearly3d_817L', 'numinstpaidearly3dest_4493216L', 'annuitynextmonth_57A', \n","#              'numinstregularpaidest_4493210L', 'firstquarter_103L', 'clientscnt_533L', 'maxdpdlast3m_392P', \n","#              'sellerplacescnt_216L', 'secondquarter_766L', 'max_periodicityofpmts_1102L', 'numinstlsallpaid_934L', \n","#              'opencred_647L', 'numinstls_657L', 'numactivecredschannel_414L', 'numinstpaidearly3d_3546850L', \n","#              'numinstpaidearlyest_4493214L', 'max_totaldebtoverduevalue_718A', 'paytype1st_925L', \n","#              'max_inittransactioncode_279L', 'max_contractst_545M', 'max_cancelreason_3545846M', \n","#              'max_rejectreason_755M', 'max_personindex_1023L', 'max_subjectroles_name_838M', 'maxdpdlast6m_474P', \n","#              'max_subjectrole_182M', 'actualdpdtolerance_344P', 'max_num_group1_9', 'max_collaterals_typeofguarante_669M', \n","#              'numinstpaidearly_338L', 'clientscnt_887L', 'maritalst_893M', 'max_subjectrole_93M', 'max_type_25L', \n","#              'max_refreshdate_3813885D', 'numinstpaidearly5d_1087L', 'max_actualdpd_943P', 'max_description_351M', \n","#              'education_88M', 'clientscnt_946L', 'clientscnt12m_3712952L', 'numactiverelcontr_750L', \n","#              'max_education_927M', 'applicationscnt_1086L', 'sellerplacecnt_915L', 'max_purposeofcred_426M', \n","#              'max_subjectroles_name_541M', 'clientscnt_1022L', 'clientscnt_360L', 'max_totaloutstanddebtvalue_668A', \n","#              'applicationscnt_629L', 'max_outstandingamount_354A', 'clientscnt_1071L', 'numactivecreds_622L', \n","#              'clientscnt_493L', 'paytype_783L', 'clientscnt6m_3712949L', 'clientscnt_304L', 'max_classificationofcontr_13M', \n","#              'numnotactivated_1143L', 'commnoinclast6m_3546845L', 'max_numberofoutstandinstls_520L', \n","#              'applicationscnt_464L', 'clientscnt_1130L', 'max_numberofoverdueinstls_834L', 'clientscnt3m_3712950L', \n","#              'max_rejectreasonclient_4145042M', 'max_contaddr_smempladdr_334L', 'numpmtchanneldd_318L', \n","#              'numcontrs3months_479L', 'max_overdueamount_31A', 'max_collaterals_typeofguarante_359M', \n","#              'clientscnt_257L', 'clientscnt_157L', 'applications30d_658L', 'clientscnt_100L', \n","#              'max_collater_typofvalofguarant_298M', 'max_pmts_month_706T', 'max_pmts_month_158T', \n","#              'mastercontrexist_109L', 'max_collater_typofvalofguarant_407M', 'mastercontrelectronic_519L', \n","#              'applicationcnt_361L', 'max_persontype_1072L', 'max_empladdr_district_926M', 'deferredmnthsnum_166L', \n","#              'max_empladdr_zipcode_114M', 'max_persontype_792L', 'max_contaddr_matchlist_1032L']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:19:53.707359Z","iopub.status.busy":"2024-03-30T16:19:53.706995Z","iopub.status.idle":"2024-03-30T16:19:53.723739Z","shell.execute_reply":"2024-03-30T16:19:53.722575Z","shell.execute_reply.started":"2024-03-30T16:19:53.707322Z"},"trusted":true},"outputs":[],"source":["# df_train = df_train.drop(drop_list)\n","# df_test = df_test.drop(drop_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Handle categorical features (Ordinal encoding)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:19:53.726141Z","iopub.status.busy":"2024-03-30T16:19:53.725294Z","iopub.status.idle":"2024-03-30T16:19:53.734651Z","shell.execute_reply":"2024-03-30T16:19:53.733593Z","shell.execute_reply.started":"2024-03-30T16:19:53.726105Z"},"trusted":true},"outputs":[],"source":["# cat_list = [col for col in df_train.columns if df_train[col].dtype.name == 'category']\n","\n","# catfreq_dict = {}\n","# catcatfreq_dict = {}\n","\n","# for col in cat_list:\n","#     catfreq_dict[col] = len(list(df_train[col].value_counts()))\n","#     catcatfreq_dict[col] = {}\n","#     for d in dict(df_train[col].value_counts()).items():\n","#         catcatfreq_dict[col][d[0]] = d[1]\n","\n","# catfreq_df = pd.DataFrame.from_dict(catfreq_dict, orient='index', columns=['Categories'])\n","# display(catfreq_df.sort_values(by=\"Categories\", ascending=False).head())\n","# display(catfreq_df.sort_values(by=\"Categories\", ascending=True).head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:19:53.736041Z","iopub.status.busy":"2024-03-30T16:19:53.735715Z","iopub.status.idle":"2024-03-30T16:19:53.750027Z","shell.execute_reply":"2024-03-30T16:19:53.748722Z","shell.execute_reply.started":"2024-03-30T16:19:53.736014Z"},"trusted":true},"outputs":[],"source":["# ordinal_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\n","# df_train[cat_list] = ordinal_enc.fit_transform(df_train[cat_list])\n","# df_test[cat_list] = ordinal_enc.transform(df_test[cat_list])\n","# df_train[cat_list].head()"]},{"cell_type":"markdown","metadata":{},"source":["### Handle NaN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:19:53.75181Z","iopub.status.busy":"2024-03-30T16:19:53.75113Z","iopub.status.idle":"2024-03-30T16:19:53.762009Z","shell.execute_reply":"2024-03-30T16:19:53.760813Z","shell.execute_reply.started":"2024-03-30T16:19:53.751781Z"},"trusted":true},"outputs":[],"source":["# nan_list = []\n","# for col, boo in df_train.isnull().any().items():\n","#     if boo == True:\n","#         nan_list.append(col)\n","\n","# print(f\"Number of col contains Nan value: {len(nan_list)}\")\n","# for i, v in df_train.isna().sum().items():\n","#     if v/len(df_train)>0.6:\n","#         print(f\"{i} : \\t {round((v/len(df_train))*100)}% Nan \")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:19:53.766645Z","iopub.status.busy":"2024-03-30T16:19:53.76629Z","iopub.status.idle":"2024-03-30T16:19:53.77403Z","shell.execute_reply":"2024-03-30T16:19:53.772761Z","shell.execute_reply.started":"2024-03-30T16:19:53.766614Z"},"trusted":true},"outputs":[],"source":["# ### trial\n","# from sklearn.impute import SimpleImputer\n","# imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n","# df_train[nan_list] = imp.fit_transform(df_train[nan_list])\n","# df_test[nan_list] = imp.transform(df_test[nan_list])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T16:19:53.776414Z","iopub.status.busy":"2024-03-30T16:19:53.776084Z","iopub.status.idle":"2024-03-30T16:19:53.7853Z","shell.execute_reply":"2024-03-30T16:19:53.7843Z","shell.execute_reply.started":"2024-03-30T16:19:53.776386Z"},"trusted":true},"outputs":[],"source":["# ## no work (too slow..) \n","# ## require dimensionality reduction first & features with high feature impor. \n","# imputer = KNNImputer()\n","# df_train = imputer.fit_transform(df_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y = df_train[\"target\"]\n","weeks = df_train[\"WEEK_NUM\"]\n","df_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n","cv = StratifiedGroupKFold(n_splits=5, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T17:30:58.472172Z","iopub.status.busy":"2024-03-30T17:30:58.471764Z"},"trusted":true},"outputs":[],"source":["\n","params = {\n","    \"boosting_type\": \"gbdt\",\n","    \"objective\": \"binary\",\n","    \"metric\": \"auc\",\n","    \"max_depth\": 10,  \n","    \"learning_rate\": 0.05,\n","    \"n_estimators\": 2000,  \n","    \"colsample_bytree\": 0.8,\n","    \"colsample_bynode\": 0.8,\n","    \"verbose\": -1,\n","    \"random_state\": 42,\n","    \"reg_alpha\": 0.1,\n","    \"reg_lambda\": 10,\n","    \"extra_trees\":True,\n","    'num_leaves':64,\n","    \"device\": device, \n","    \"verbose\": -1,\n","}\n","\n","fitted_models = []\n","cv_scores = []\n","\n","\n","for idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#   Because it takes a long time to divide the data set, \n","    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# each time the data set is divided, two models are trained to each other twice, which saves time.\n","    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n","    model = lgb.LGBMClassifier(**params)\n","    model.fit(\n","        X_train, y_train,\n","        eval_set = [(X_valid, y_valid)],\n","        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(100)] )\n","    fitted_models.append(model)\n","    y_pred_valid = model.predict_proba(X_valid)[:,1]\n","    auc_score = roc_auc_score(y_valid, y_pred_valid)\n","    cv_scores.append(auc_score)\n","    \n","print(\"CV AUC scores: \", cv_scores)\n","print(\"Maximum CV AUC score: \", max(cv_scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-30T17:25:14.054024Z","iopub.status.idle":"2024-03-30T17:25:14.054446Z","shell.execute_reply":"2024-03-30T17:25:14.054269Z","shell.execute_reply.started":"2024-03-30T17:25:14.054253Z"},"trusted":true},"outputs":[],"source":["class VotingModel(BaseEstimator, RegressorMixin):\n","    def __init__(self, estimators):\n","        super().__init__()\n","        self.estimators = estimators\n","        \n","    def fit(self, X, y=None):\n","        return self\n","    \n","    def predict(self, X):\n","        y_preds = [estimator.predict(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)\n","    \n","    def predict_proba(self, X):\n","        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)\n","\n","model = VotingModel(fitted_models)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-30T17:25:14.056419Z","iopub.status.idle":"2024-03-30T17:25:14.056864Z","shell.execute_reply":"2024-03-30T17:25:14.056673Z","shell.execute_reply.started":"2024-03-30T17:25:14.056653Z"},"trusted":true},"outputs":[],"source":["lgb.plot_importance(fitted_models[2], importance_type=\"split\", figsize=(10,50))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-30T17:25:14.058353Z","iopub.status.idle":"2024-03-30T17:25:14.058746Z","shell.execute_reply":"2024-03-30T17:25:14.058586Z","shell.execute_reply.started":"2024-03-30T17:25:14.058571Z"},"trusted":true},"outputs":[],"source":["features = X_train.columns\n","importances = fitted_models[2].feature_importances_\n","feature_importance = pd.DataFrame({'importance':importances,'features':features}).sort_values('importance', ascending=False).reset_index(drop=True)\n","feature_importance\n","\n","drop_list = []\n","for i, f in feature_importance.iterrows():\n","    if f['importance']<80:\n","        drop_list.append(f['features'])\n","print(f\"Number of features which are not important: {len(drop_list)} \")\n","\n","print(drop_list)"]},{"cell_type":"markdown","metadata":{},"source":["### Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-30T17:25:14.05997Z","iopub.status.idle":"2024-03-30T17:25:14.060341Z","shell.execute_reply":"2024-03-30T17:25:14.060185Z","shell.execute_reply.started":"2024-03-30T17:25:14.06017Z"},"trusted":true},"outputs":[],"source":["df_test = df_test.drop(columns=[\"WEEK_NUM\"])\n","df_test = df_test.set_index(\"case_id\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-30T17:25:14.061655Z","iopub.status.idle":"2024-03-30T17:25:14.062061Z","shell.execute_reply":"2024-03-30T17:25:14.061897Z","shell.execute_reply.started":"2024-03-30T17:25:14.061881Z"},"trusted":true},"outputs":[],"source":["y_pred = pd.Series(model.predict_proba(df_test)[:, 1], index=df_test.index)\n","df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n","df_subm = df_subm.set_index(\"case_id\")\n","\n","df_subm[\"score\"] = y_pred\n","df_subm.to_csv(\"submission.csv\")\n","df_subm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-30T17:25:14.063693Z","iopub.status.idle":"2024-03-30T17:25:14.064114Z","shell.execute_reply":"2024-03-30T17:25:14.063947Z","shell.execute_reply.started":"2024-03-30T17:25:14.063931Z"},"trusted":true},"outputs":[],"source":["#X_test = df_test.drop(columns=[\"WEEK_NUM\"])\n","#X_test = X_test.set_index(\"case_id\")\n","\n","#lgb_pred = pd.Series(model.predict_proba(X_test)[:, 1], index=X_test.index)\n","\n","#df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n","#df_subm = df_subm.set_index(\"case_id\")\n","\n","#df_subm[\"score\"] = lgb_pred\n","\n","#df_subm.head()\n","\n","#df_subm.to_csv(\"submission.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["* handle nan value (both numeric & categorical)\n","* preprocess the minority categorical value (drop / keep)\n","* float to int after ordinal encode\n","* oversampling (smote? (considering date feature)\n","* high dimension (pca) \n","* create new features based on high fea_imp features\n","...\n","..."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-30T17:25:14.066022Z","iopub.status.idle":"2024-03-30T17:25:14.067083Z","shell.execute_reply":"2024-03-30T17:25:14.066874Z","shell.execute_reply.started":"2024-03-30T17:25:14.066853Z"},"trusted":true},"outputs":[],"source":["# X_resampled, y_resampled = SMOTE().fit_resample(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
